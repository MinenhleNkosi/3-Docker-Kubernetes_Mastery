# 1. Use of Containers in Cloud Native ğŸ“¦
The journey of application development has always been closely tied to how we **package** and **deploy** software â€” and `containers` are the latest leap in that evolution ğŸš€.

## ğŸ Example: A Simple C# Web App
Letâ€™s say Iâ€™ve built a small web app in C#. To run it, Iâ€™d typically need:

    ğŸ–¥ï¸ A base operating system.

    ğŸ C# and .NET Core packages installed.

    ğŸ“¦ Additional libraries (e.g., Dapper, EF Core).

    ğŸŒ Networking set up.

    ğŸ—„ï¸ Connections to databases, caches, or storage systems.

Traditionally, **system administrators** would set up the environment to meet all these requirements â€” which can be:

* â— Error-prone
  
* ğŸ” Time-consuming
  
* ğŸ“‰ Hard to maintain

So, servers were usually set up for one specific purpose, like running just a `database` or `app server`, and then connected over the network.

<a ><br /><img src="https://app.eraser.io/workspace/PdEX5bJXiE6yVrjaLFCq/preview?elements=x08hfMs_YSG7fYn-HxwZew&type=embed" /></a>

<br />

## ğŸ§® Enter: Virtual Machines (VMs)
To make better use of hardware, **Virtual Machines** came into play:

* Emulate a full computer (`OS`, `CPU`, `RAM`, etc.)
  
* Allow multiple isolated environments on the same server.
  
* Great for running different apps side-by-side.

**But VMs have a downside**:
* âš ï¸ Theyâ€™re heavyweight â€” each one needs its own full OS kernel, which adds overhead.
  
* ğŸ³ The Container Revolution

**`Containers` solve both major problems**:
* âœ… Package the app + all dependencies into one portable unit.
  
* âš¡ Run more efficiently than VMs â€” no need for a full **OS** per app.

**`Containers` share the host `OS kernel` but remain isolated, making them**:
* ğŸª¶ Lightweight.
  
* ğŸ”„ Easy to replicate.
  
* ğŸšš Portable across environments (dev, staging, prod).

## ğŸ§° Why Containers Matter
**Containers make it easier for developers to**:
* Define their appâ€™s **environment** in a `single container image`.
  
* Ensure **consistency** across machines.
  
* Focus on writing code, not configuring servers.

**And for operations teams, containers mean**:
* More **efficient** resource usage ğŸ’°
  
* Easier **updates** and **rollbacks** ğŸ”„
  
* Simpler scaling âš–ï¸

By using **containers**, I can **build**, **ship**, and **run** my applications anywhere, with confidence and consistency ğŸŒ.

The diagram below illustarte the **difference** between **Docker images** and **VMs**: 

<a><br /><img src="https://app.eraser.io/workspace/PdEX5bJXiE6yVrjaLFCq/preview?elements=Fs2hLwEtPIwPM7rSRgDnlw&type=embed" /></a>

<br />

# 2. Running Containers with Docker & Beyond ğŸ³
**What Does It Mean to Run a Container?**

You might think you need Docker to run containers... but surprise! ğŸ˜²
Thereâ€™s actually a standard for this:

    OCI (Open Container Initiative) sets the rules with:

        ğŸ“¦ image-spec â€“ how images are built

        âš™ï¸ runtime-spec â€“ how containers run

Think of:

    A container image as a class ğŸ§±
    A running container as an object (instance) ğŸƒ

ğŸ§° **The Tools Behind the Scenes**

    ğŸ”§ runC â€“ The engine under the hood. Itâ€™s a low-level runtime used by Docker, Podman, and others.


    ğŸ³ Docker uses runC behind the scenes. You can run containers like this:

docker run nginx

# 3. Building Container Images with Docker ğŸ“¦
So, **why is it called a container**? ğŸ¤”
It's based on the idea of **shipping containers** â€” **standardized**, **stackable**, and **portable**. ğŸš¢
Just like those metal boxes can carry goods **anywhere**, `container images` carry your **app** across systems!

## What Makes Containers Portable? ğŸ§±
Docker brought together tools like:

* ğŸ§© **Namespaces** (isolation)

* ğŸ”’ **cgroups** (resource limits)

But the game-changer was ğŸ“¦ `container images`.

* A container image is like a **complete app-in-a-box**:

        Code ğŸ§‘â€ğŸ’»

        Runtime ğŸ› ï¸

        System tools âš™ï¸

        Libraries ğŸ“š

        Settings ğŸ”§


## The OCI Standard ğŸŒ
In 2015, Dockerâ€™s image format was donated to the **Open Container Initiative (OCI)**.

Today, it's called the **OCI image-spec** and lives on GitHub. ğŸ™

An image = ğŸ—‚ Filesystem bundle + ğŸ“ Metadata

## Building an Image with a Dockerfile ğŸ—ï¸
You create container images with a special file called a **Dockerfile**.
It's like a recipe ğŸ“ƒ for setting up your appâ€™s environment.

Letâ€™s containerize a simple **C# console app!** ğŸ’»

ğŸ§ª Sample Dockerfile for a C# App:

```Dockerfile
# Use the official .NET SDK image to build the app
FROM mcr.microsoft.com/dotnet/sdk:6.0 AS build

# Set the working directory inside the container
WORKDIR /app

# Copy the C# project files
COPY . .

# Restore and build the app
RUN dotnet restore
RUN dotnet publish -c Release -o out

# Use the runtime image for smaller final size
FROM mcr.microsoft.com/dotnet/runtime:6.0

# Set the working directory again
WORKDIR /app

# Copy the published output from the build stage
COPY --from=build /app/out .

# Define the entrypoint (this runs when container starts)
CMD ["dotnet", "MyApp.dll"]
```

## Building & Running the Image ğŸ› ï¸
If Docker is installed, you can build your image like this:

```bash
docker build -t my-csharp-image -f Dockerfile .
```

* `-t` gives your image a name.

* `-f` tells Docker where to find your Dockerfile.

* `.` is the build context (your current folder).

## Sharing Your Image
Once built, you can **push it to a registry** (like GitHub Container Registry, Docker Hub, or a private one):

```bash
docker push my-registry.com/my-csharp-image
docker pull my-registry.com/my-csharp-image
```

# 4. Securing Your Containers ğŸ” 
Security is not optional when working with containers. Letâ€™s explore how containers differ from virtual machines and what you need to protect. ğŸ›¡ï¸

## Containers vs. Virtual Machines
Containers are **not VMs** âš ï¸

They share the **same kernel** ğŸ§  with the host, which means:

    If one container breaks the rules, it can mess with the whole system! ğŸ’¥

For example, if a container:

    Kills processes ğŸª“

    Adds routing rules ğŸ›£ï¸

    Modifies the host network ğŸŒ
    â€¦it can become a serious threat.

ğŸ“ Learn more about this in the [Docker capabilities docs](https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities)

## Beware of Running as Root ğŸ§‘â€ğŸ’»
One of the biggest mistakes in container security:

    âŒ Running apps as root (admin) inside containers

This gives the app way too much power.
Unfortunately, many public images still run as root by default. ğŸ˜“

âœ… Best practice:
Use a non-root user in your Dockerfile like this:

```dockerfile
# Create a new user
RUN useradd -m appuser
USER appuser
```

## Danger of Public Images ğŸŒ
Pulling images from public registries like:

    ğŸ³ Docker Hub

    ğŸ§º Quay

...can expose your system to **malware** if you're not careful.

ğŸ•µï¸ Always:

    Check image source ğŸ‘€

    Use official or verified images âœ…

    Pin image versions ğŸ”’ (node:18.14.2, not just node:latest)

ğŸ“š Recommended read: [Sysdig's guide to secure image building](https://sysdig.com/blog/)

## The 4 C's of Cloud Native Security ğŸ›¡ï¸
To build a strong security model, think in layers:

1. **Cloud** â˜ï¸ â€“ Your cloud provider & infrastructure

2. **Cluster** ğŸ§± â€“ Kubernetes, nodes, control plane

3. **Container** ğŸ“¦ â€“ Images, runtime, file permissions

4. **Code** ğŸ’» â€“ Application logic, dependencies

    ğŸ” Each layer protects the one inside it.
    So... donâ€™t skip a layer!

Learn more in the [Kubernetes Security Docs](https://kubernetes.io/docs/concepts/security/overview/)

## ğŸŒ€ Security is a Journey

Thereâ€™s no single fix for container security.
Itâ€™s an ongoing process of best practices, audits, and continuous learning.

# 5. Container Orchestration Fundamentals ğŸ¤–
Running a few containers locally? Easy.
Running **hundreds of them** across servers? Not so easy. ğŸ˜…
Thatâ€™s where **container orchestration** comes in! ğŸ› ï¸ğŸ“¦

## ğŸ§© The Rise of Microservices
Modern apps are no longer **monoliths** ğŸ° â€” they are built from small, self-contained services called **microservices** ğŸ§±.

Each microservice is:

    ğŸš¢ Containerized

    ğŸ§ Independent

    ğŸ”„ Loosely coupled

Put them together, and youâ€™ve got a **microservice architecture** â€” a modular way to build modern apps.

## âš™ï¸ Why Orchestration?
As you scale, you face new challenges:

âœ… What needs to be managed:

* ğŸ–¥ï¸ **Compute**: Where do containers run?

* ğŸ§® **Scheduling**: How are containers spread across servers?

* ğŸ”‹ **Resource allocation**: CPU & memory per container?

* ğŸ’€ **Resilience**: What if a container fails?

* ğŸ“ˆ **Scaling**: What if traffic spikes?

* ğŸŒ **Networking**: How do containers talk to each other?

* ğŸ’¾ **Storage**: What if data must persist?

Itâ€™s just too much to do manually... ğŸ˜°

## What is Container Orchestration? ğŸ› ï¸

    A container orchestration system is your robot manager ğŸ¤–
    It automates everything about running containers at scale.

These systems create a `cluster` â€” a **group of servers** that:

* ğŸ“¡ Use a **control plane** (orchestrates everything)

* ğŸ› ï¸ Use **worker nodes** (run the actual containers)

Together, they help manage container lifecycles across large systems.

## The Industry Standard â€“ Kubernetes ğŸŒŸ
There have been other systems in the past (like **Docker Swarm**, **Mesos**, **Nomad**)...
But today, one tool rules them all:

* ğŸ§  **Kubernetes** = Industry-standard container orchestrator

Built by Google, adopted by the CNCF, and supported everywhere ğŸŒ.

Kubernetes lets you:

* ğŸ” Automatically **restart** failed containers.

* â¬†ï¸ Scale **up/down** based on demand.

* âš–ï¸ **Distribute workloads** efficiently.

* ğŸ” **Secure**, **monitor**, and **connect containers** seamlessly.

## ğŸ“Œ FYI:
* **Microservices** = lots of small containers ğŸ§±

* **Orchestration** = automated management ğŸ› ï¸

* **Kubernetes** = the go-to solution ğŸ§ 

<a><br /><img src="https://app.eraser.io/workspace/PdEX5bJXiE6yVrjaLFCq/preview?elements=3nwMqRlHVc-8K5zWdJn5mQ&type=embed" /></a>

<br />

# 6. Networking in the Container World ğŸŒ 
In a microservice architecture, **communication is everything** ğŸ—£ï¸.
Unlike monoliths, microservices **talk to each other** over the network.
Letâ€™s dive into how that works with containers! ğŸ³ğŸ”Œ

## ğŸ§© Microservices Need Networking
Each microservice is like its own mini-app ğŸ§±.
To function, it needs to **expose an interface** (usually an HTTP API):

ğŸ“¦ Example:

* `/products` â†’ Returns a list of products in your app ğŸ›ï¸

But how do containers make all this possible?

## ğŸ•¸ï¸ Isolated Networks for Each Container
Thanks to **network namespaces**, each container gets:

* ğŸ†” A unique IP address

* ğŸ› ï¸ Its own network stack

This means you can have:

* Multiple containers **using the same port** (e.g., port 8080)

* No conflict because each container is in its own little network bubble ğŸ«§

## ğŸ” Making Containers Reachable
To expose a container to the *outside world*, we use **port mapping** ğŸ”:

```bash
docker run -p 8080:80 my-container
```

This maps:

    Port 80 inside the container â†’ Port 8080 on your machine
    Now you can access the app at localhost:8080 ğŸŒ

## ğŸŒ Connecting Containers Across Hosts
When containers are spread across multiple servers, we use an **overlay network** ğŸ•¸ï¸:

* It acts like a **virtual network across hosts**.

* Containers on different machines can **talk like theyâ€™re on the same LAN**.

No need for complex manual setups ğŸš«
Overlay networks handle:

* ğŸ§  IP address assignment

* ğŸš¦ Routing traffic between containers

* ğŸ›¡ï¸ Simplifying network security

Popular container orchestration platforms (like Kubernetes) often come with overlay networking built-in.

## ğŸ“Œ FYI:
* ğŸ§  Microservices talk via **APIs**

* ğŸ› ï¸ Containers get their own **IP** + **ports**

* ğŸŒ Use port mapping to expose services

* ğŸ•¸ï¸ Overlay networks connect containers across servers

# 7. Service Discovery & DNS ğŸ§­ 
Gone are the days when sysadmins memorized server IPs ğŸ§ ğŸ’¾.
Welcome to the world of **containers** â€” **dynamic**, **scalable**, andâ€¦ kinda chaotic without proper networking tools. Letâ€™s fix that! ğŸ§°ğŸŒ

## ğŸ¢ Old-School Server Management
In traditional data centers:

* ğŸ‘¨â€ğŸ’» **Admins memorized IPs** or used **static lists**.

* ğŸ“’ Servers had **fixed names**, **addresses**, and **roles**.

* ğŸ”— Everything was connected by **manual configuration**.

But now? Weâ€™re dealing with **containers**... and things get wild ğŸŒ€

## ğŸ“¦ The Container Chaos
With container orchestration:

* You may have **hundreds or thousands** of containers ğŸ˜±

* Containers can live:

    On different **hosts** ğŸ–¥ï¸

    In different **regions** ğŸŒ

    Inside **different data centers** ğŸ¢

* They get new IPs every time they restart or move

* Containers often appear and disappear like magic ğŸª„

Clearly, IP-based networking doesnâ€™t scale here.

## ğŸ§™â€â™‚ï¸ Enter Service Discovery

* ğŸ’¡ **Service Discovery** = Automatically finding services by **name**, not IP

Instead of memorizing addresses, services now register themselves in a **Service Registry** ğŸ—‚ï¸.

When a container starts, it tells the system:
    "Hey, Iâ€™m alive! You can reach me at this address!" ğŸ“£
    When it stops:
    "Iâ€™m gone now! Clean me up!" ğŸ§¹

This keeps the **service directory** fresh and reliable âœ…

## ğŸŒ DNS(Domain Name System) to the Rescue
Most platforms (like Kubernetes) use DNS-**based service discovery** ğŸ§­:

* Each service gets a **DNS name** (e.g., `product-service.default.svc.cluster.local`)

* When containers want to talk, they use the **name**, not the IP

* DNS automatically resolves the name to the **current IP** of the container ğŸ§ â¡ï¸ğŸ“¦

This makes communication **simple** and **reliable**, even in **fast-changing** environments.

## ğŸ“Œ FYI:

* ğŸ’¾ Manual IP tracking is **outdated**.

* ğŸ“¦ **Containers change IPs often** â€” too fast to track manually.

* ğŸ§­ **Service Discovery** helps locate **services** dynamically

* ğŸŒ **DNS** lets containers talk by **name**, **not number**.

# 8. What is a Service Mesh?
As container-based applications grow in complexity, so do their **networking needs** ğŸŒ. Developers want more than just connectivity â€” they want **security**, **monitoring**, and **control** over how services talk to each other. That's where **service mesh** steps in! ğŸš€

## ğŸ§± The Problem
Modern microservices need more than just "networking":

* ğŸ” **Encryption** between services.

* ğŸ›ï¸ **Traffic control** (retries, timeouts, rate-limiting).

* ğŸ‘€ **Monitoring** and **tracing**.

* ğŸ”’ **Access** control.

But building all that into every service? ğŸ˜©
Too much complexity, too much duplicated code.

## ğŸ§™â€â™‚ï¸ Enter the Proxy
Instead of adding all that logic into your app, why not let a **proxy** do the work?

ğŸ§Š A **proxy** is like a gatekeeper:

* It sits between the client and the server

* It can inspect, modify, encrypt, or block traffic

Examples:

* ğŸ” `nginx`

* ğŸš¦ `haproxy`

* ğŸ›°ï¸ `envoy`

## What is a Service Mesh?
A **service mesh** is a system where **every service** in your architecture gets its own **proxy container**.

ğŸ’¡ Instead of apps talking directly, they go through their local proxy:

```css
App A --> Proxy A --> Proxy B --> App B
```

These proxies form the **data plane**:
ğŸ¯ Itâ€™s where actual traffic flows and rules are enforced.

## ğŸ® The Control Plane
You manage all the proxy rules in one central place: the **control plane** ğŸ§ 

ğŸ“„ Example:

    "Make all traffic between Service A and B encrypted using TLS."

Just write that **rule** in a **config file**, send it to the **control plane**, and it gets applied **automatically** to all **proxies** in the **mesh** ğŸ”„

## ğŸ”¥ Example Use Case: Encryption

Without a service mesh:

* You write code to manage encryption

* You install libraries and manage certificates ğŸ§¾ğŸ”

With a service mesh:

* No code changes

* Just write a rule:

```yaml
encrypt: true
from: service-a
to: service-b
```

* Mesh handles the rest âœ…

## ğŸŒŸ Popular Service Mesh Tools

* ğŸ›°ï¸ `Istio`

* ğŸ§© `Linkerd`

Both follow the same basic architecture:

* ğŸ“¡ Proxies = **data plane**.

* ğŸ§  Controller = **control plane**.

## ğŸ“Œ FYI

* Service mesh simplifies complex networking âœ¨

* Uses **proxies** to handle traffic between containers.

* **Features**: *encryption*, *monitoring*, *retries*, *routing*.

* Managed by a **control plane** with easy config files

* Tools like `Istio` and `Linkerd` bring it all together

# Storage in Containers ğŸ“¦ 

## ğŸŒªï¸ The Ephemeral Problem
Containers are **ephemeral** â€” which means:

* ğŸ§¼ When they go, their data goes too!

When you start a container:

* âœ… It uses a **read-only** image.

* â• A **read-write** layer is added on top (temporary!).

* âŒ When the **container stops**, the write layer is **deleted**.

So... where does your data go? ğŸ’¥ Gone. Unless you do something about it.

## ğŸ§± Volumes to the Rescue
To persist data, we use **volumes**!

ğŸ“ A **volume** is a **directory on the host machine** that is:

* ğŸ“¥ Mounted into the container.

* ğŸ”„ Used to read/write data **outside** of the container's short life.

```bash
docker run -v /host/data:/container/data my-app
```

âš ï¸ But careful â€” this gives the container **access to the host filesystem**, which weakens isolation ğŸ§ 

## ğŸ¤ Sharing Data Between Containers
Sometimes, you want **multiple containers** on the same host to:

* ğŸ”„ Share files

* ğŸ—ƒï¸ Access the same database files

* ğŸ–¼ï¸ Share logs or uploads

âœ… Volumes can help â€” theyâ€™re shared between containers as long as they're on the same host.

## ğŸ›°ï¸ Scaling the Challenge
Uh-oh. In **container orchestration** (like `Kubernetes`), things get trickier:

* Containers move between **different hosts** ğŸ”

* Hosts may be in **different regions** ğŸ—ºï¸

* Data still needs to be available wherever the container goes ğŸ“¡

So local volumes alone wonâ€™t cut it.

## ğŸ§© Kubernetes + CSI
ğŸ’¡ Kubernetes helps manage storage at scale â€” but it needs a common language to work with all kinds of storage.

That's where **CSI** (**Container Storage Interface**) comes in:

* ğŸ”— A **standard API** for connecting containers to **any storage** backend

* â˜ï¸ Works with **cloud** (e.g. `AWS EBS`, `GCP PD`)

* ğŸ¢ Or with **on-premises** systems

* ğŸ“¦ Kubernetes uses it to attach volumes, even across nodes

## ğŸ“Œ FYI:

* **Containers** = temporary ğŸ•

* **Volumes** = host-mounted folders that keep your data safe ğŸ’¾

* For shared or multi-host setups, you need persistent storage

* **CSI** standard makes storage plug-and-play across systems ğŸ”Œ

<br />
<a ><br /><img src="https://app.eraser.io/workspace/Mh6RNld4rNvXQJmTbcJR/preview?elements=tgrd19M44MU31Cv3_Q9vjA&type=embed" /></a>
